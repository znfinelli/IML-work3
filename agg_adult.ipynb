{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c69b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Utilities\n",
    "from utils.parser import preprocess_single_arff\n",
    "from utils.clustering_metrics import compute_clustering_metrics\n",
    "\n",
    "# Session 1 Algorithms\n",
    "from algorithms.agg_clustering import run_agglomerative_once\n",
    "from algorithms.gmm_clustering import run_gmm_once\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ---------------------------------------------------------\n",
    "RUN_CONFIG = {\n",
    "    \"datasets\": {\n",
    "        \"pen-based\": False,\n",
    "        \"adult\": True,\n",
    "        \"mushroom\": False\n",
    "    },\n",
    "    \"algorithms\": {\n",
    "        \"Agglomerative\": True,\n",
    "        \"GMM\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "DATASETS_MAP = {\n",
    "    \"pen-based\": \"datasets/pen-based.arff\",\n",
    "    \"adult\": \"datasets/adult.arff\",\n",
    "    \"mushroom\": \"datasets/mushroom.arff\",\n",
    "}\n",
    "\n",
    "# Global Parameters\n",
    "N_CLUSTERS_LIST = list(range(2, 11))\n",
    "\n",
    "# Distances\n",
    "METRICS = [\"euclidean\", \"manhattan\", \"cosine\"]\n",
    "\n",
    "# Initialization methods\n",
    "GMM_INIT_PARAMS = [\"kmeans\", \"random\", \"k-means++\", \"random_from_data\"]\n",
    "\n",
    "N_RUNS = 10\n",
    "PARTIAL_SAVE_INTERVAL = 2\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------------------------------------\n",
    "def generate_task_list():\n",
    "    tasks = []\n",
    "    for ds_name, ds_enabled in RUN_CONFIG[\"datasets\"].items():\n",
    "        if not ds_enabled: continue\n",
    "\n",
    "        # 1. Agglomerative Tasks\n",
    "        if RUN_CONFIG[\"algorithms\"][\"Agglomerative\"]:\n",
    "            for k in N_CLUSTERS_LIST:\n",
    "                for link in [\"complete\", \"average\", \"single\"]:\n",
    "                    for metric in METRICS:\n",
    "                        tasks.append({\n",
    "                            \"type\": \"agg\",\n",
    "                            \"dataset\": ds_name,\n",
    "                            \"n_clusters\": k,\n",
    "                            \"linkage\": link,\n",
    "                            \"metric\": metric\n",
    "                        })\n",
    "\n",
    "        # 2. GMM Tasks\n",
    "        if RUN_CONFIG[\"algorithms\"][\"GMM\"]:\n",
    "            for k in N_CLUSTERS_LIST:\n",
    "                for init_p in GMM_INIT_PARAMS:\n",
    "                    # We perform N_RUNS for each configuration\n",
    "                    for seed in range(N_RUNS):\n",
    "                        tasks.append({\n",
    "                            \"type\": \"gmm\",\n",
    "                            \"dataset\": ds_name,\n",
    "                            \"n_clusters\": k,\n",
    "                            \"init_params\": init_p,\n",
    "                            \"run_id\": seed\n",
    "                        })\n",
    "    return tasks\n",
    "\n",
    "\n",
    "def save_dataframe(data, folder, filename):\n",
    "    # Check if the input is a pandas DataFrame\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        if data.empty:\n",
    "            return\n",
    "        df_to_save = data\n",
    "    # Check if the input is an empty list/dictionary\n",
    "    elif not data:\n",
    "        return\n",
    "    # If it's not a DataFrame and not empty, assume it's a list of records\n",
    "    else:\n",
    "        df_to_save = pd.DataFrame(data)\n",
    "\n",
    "    # Ensure the directory exists before attempting to save\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Save the file\n",
    "    df_to_save.to_csv(os.path.join(folder, filename), index=False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------------------------------------\n",
    "def main():\n",
    "    session_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_dir = f\"results_session1/run_{session_id}\"\n",
    "    dirs = [\n",
    "        base_dir,\n",
    "        os.path.join(base_dir, \"partial\"),\n",
    "        os.path.join(base_dir, \"by_dataset\"),\n",
    "        os.path.join(base_dir, \"by_algorithm\")\n",
    "    ]\n",
    "    for d in dirs: os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    print(f\"Session 1 Runner Started: {session_id}\")\n",
    "    print(\"Generating task list...\")\n",
    "    all_tasks = generate_task_list()\n",
    "\n",
    "    if not all_tasks:\n",
    "        print(\"No tasks configured.\")\n",
    "        return\n",
    "\n",
    "    global_results = []\n",
    "    current_ds_results = []\n",
    "    current_ds_name = None\n",
    "    X, y = None, None\n",
    "\n",
    "    pbar = tqdm(all_tasks, unit=\"exp\")\n",
    "\n",
    "    for i, task in enumerate(pbar):\n",
    "        ds_name = task[\"dataset\"]\n",
    "        desc = f\"{ds_name} | {task.get('type')} | k={task['n_clusters']}\"\n",
    "        pbar.set_description(f\"{desc:<45}\")\n",
    "\n",
    "        # Load Data\n",
    "        if ds_name != current_ds_name:\n",
    "            if current_ds_name and current_ds_results:\n",
    "                save_dataframe(current_ds_results, dirs[2], f\"{current_ds_name}_results.csv\")\n",
    "                current_ds_results = []\n",
    "            try:\n",
    "                X, y, _ = preprocess_single_arff(DATASETS_MAP[ds_name], drop_class=False)\n",
    "                current_ds_name = ds_name\n",
    "            except Exception as e:\n",
    "                pbar.write(f\"Error loading {ds_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        res = {}\n",
    "\n",
    "        try:\n",
    "            if task[\"type\"] == \"agg\":\n",
    "                res = run_agglomerative_once(\n",
    "                    X, task[\"n_clusters\"], task[\"metric\"], task[\"linkage\"], ds_name\n",
    "                )\n",
    "            elif task[\"type\"] == \"gmm\":\n",
    "                res = run_gmm_once(X, task[\"n_clusters\"], task[\"init_params\"], ds_name)\n",
    "                res[\"run_id\"] = task[\"run_id\"]\n",
    "\n",
    "            # Metrics\n",
    "            res[\"runtime\"] = time.perf_counter() - start_time\n",
    "            if y is not None and \"labels\" in res:\n",
    "                res.update(compute_clustering_metrics(X, y, res[\"labels\"]))\n",
    "                del res[\"labels\"]\n",
    "\n",
    "            global_results.append(res)\n",
    "            current_ds_results.append(res)\n",
    "\n",
    "        except Exception as e:\n",
    "            pbar.write(f\"Task failed: {task} Error: {e}\")\n",
    "\n",
    "        # Partial Save\n",
    "        if (i + 1) % PARTIAL_SAVE_INTERVAL == 0:\n",
    "            save_dataframe(global_results, dirs[1], f\"partial_{session_id}.csv\")\n",
    "\n",
    "    # Final Saves\n",
    "    if current_ds_results:\n",
    "        save_dataframe(current_ds_results, dirs[2], f\"{current_ds_name}_results.csv\")\n",
    "\n",
    "    if global_results:\n",
    "        df_final = pd.DataFrame(global_results)\n",
    "        df_final.to_csv(os.path.join(base_dir, \"session1_final_results.csv\"), index=False)\n",
    "\n",
    "        for algo in df_final['algorithm'].unique():\n",
    "            safe_name = algo.replace(\" \", \"_\")\n",
    "            save_dataframe(df_final[df_final['algorithm'] == algo], dirs[3], f\"{safe_name}.csv\")\n",
    "\n",
    "        print(f\"\\nSession 1 Complete. Results in {base_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
