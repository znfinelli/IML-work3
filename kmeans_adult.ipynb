{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Utilities\n",
    "from utils.parser import preprocess_single_arff\n",
    "from utils.clustering_metrics import compute_clustering_metrics\n",
    "\n",
    "# Session 2 Algorithms\n",
    "from algorithms.kmeans import KMeans\n",
    "from algorithms.kmeansfekm import KMeansFEKM\n",
    "from algorithms.kernel_kmeans import KernelKMeans\n",
    "from algorithms.fuzzy_c_means import FuzzyCMeans\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ---------------------------------------------------------\n",
    "RUN_CONFIG = {\n",
    "    \"datasets\": {\n",
    "        \"pen-based\": False,\n",
    "        \"adult\": True,\n",
    "        \"mushroom\": False\n",
    "    },\n",
    "    \"algorithms\": {\n",
    "        \"KMeans_Variants\": True,\n",
    "        \"Fuzzy_Clustering\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "DATASETS_MAP = {\n",
    "    \"pen-based\": \"datasets/pen-based.arff\",\n",
    "    \"adult\": \"datasets/adult.arff\",\n",
    "    \"mushroom\": \"datasets/mushroom.arff\",\n",
    "}\n",
    "\n",
    "N_CLUSTERS_LIST = list(range(2, 11))\n",
    "METRICS = [\"euclidean\", \"manhattan\"]\n",
    "FUZZY_M = [1.5, 2.0, 2.5]\n",
    "N_RUNS = 10\n",
    "PARTIAL_SAVE_INTERVAL = 2\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------------------------------------\n",
    "def generate_task_list():\n",
    "    tasks = []\n",
    "    for ds_name, ds_enabled in RUN_CONFIG[\"datasets\"].items():\n",
    "        if not ds_enabled: continue\n",
    "\n",
    "        # 3. K-Means Variants Tasks\n",
    "        if RUN_CONFIG[\"algorithms\"][\"KMeans_Variants\"]:\n",
    "            km_algos = [\n",
    "                (\"KMeans_Standard\", KMeans),\n",
    "                (\"KMeans_FEKM\", KMeansFEKM),\n",
    "                (\"Kernel_KMeans\", KernelKMeans)\n",
    "            ]\n",
    "            for algo_name, AlgoClass in km_algos:\n",
    "                for k in N_CLUSTERS_LIST:\n",
    "\n",
    "                    # Logic to handle different parameters for Kernel vs Standard\n",
    "                    if algo_name == \"Kernel_KMeans\":\n",
    "                        # Kernel K-Means uses 'kernel' parameter, not 'metric'\n",
    "                        # We test RBF kernel as the standard non-linear approach\n",
    "                        current_metrics = [\"rbf\"]\n",
    "                    else:\n",
    "                        # Standard/FEKM use Euclidean/Manhattan\n",
    "                        current_metrics = METRICS\n",
    "\n",
    "                    for metric in current_metrics:\n",
    "                        # Optimization: FEKM and Kernel (Intelligent) are deterministic.\n",
    "                        # Running them 10 times produces the exact same result, so we run once.\n",
    "                        if algo_name in [\"KMeans_FEKM\", \"Kernel_KMeans\"]:\n",
    "                            current_runs = 1\n",
    "                        else:\n",
    "                            current_runs = N_RUNS\n",
    "\n",
    "                        for seed in range(current_runs):\n",
    "                            task = {\n",
    "                                \"type\": \"kmeans\",\n",
    "                                \"class\": AlgoClass,\n",
    "                                \"algo_name\": algo_name,\n",
    "                                \"dataset\": ds_name,\n",
    "                                \"n_clusters\": k,\n",
    "                                \"run_id\": seed\n",
    "                            }\n",
    "\n",
    "                            # Assign the correct parameter name\n",
    "                            if algo_name == \"Kernel_KMeans\":\n",
    "                                task[\"kernel\"] = metric  # e.g. 'rbf'\n",
    "                            else:\n",
    "                                task[\"metric\"] = metric  # e.g. 'euclidean'\n",
    "\n",
    "                            tasks.append(task)\n",
    "\n",
    "        # 4. Fuzzy Clustering Tasks\n",
    "        if RUN_CONFIG[\"algorithms\"][\"Fuzzy_Clustering\"]:\n",
    "            # alpha=1.0 is Standard Bezdek, alpha<1.0 is Suppressed Fan et al.\n",
    "            alphas = [1.0, 0.75, 0.5]\n",
    "            for k in N_CLUSTERS_LIST:\n",
    "                for m in FUZZY_M:\n",
    "                    for alpha in alphas:\n",
    "                        algo_name = \"FCM_Standard\" if alpha == 1.0 else f\"FCM_Suppressed_{alpha}\"\n",
    "                        for seed in range(N_RUNS):\n",
    "                            tasks.append({\n",
    "                                \"type\": \"fuzzy\",\n",
    "                                \"dataset\": ds_name,\n",
    "                                \"algo_name\": algo_name,\n",
    "                                \"n_clusters\": k,\n",
    "                                \"m\": m,\n",
    "                                \"alpha\": alpha,\n",
    "                                \"run_id\": seed\n",
    "                            })\n",
    "    return tasks\n",
    "\n",
    "\n",
    "def save_dataframe(data, folder, filename):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        if data.empty: return\n",
    "        df_to_save = data\n",
    "    elif not data:\n",
    "        return\n",
    "    else:\n",
    "        df_to_save = pd.DataFrame(data)\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    df_to_save.to_csv(os.path.join(folder, filename), index=False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------------------------------------\n",
    "def main():\n",
    "    session_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_dir = f\"results_session2/run_{session_id}\"\n",
    "    dirs = [\n",
    "        base_dir,\n",
    "        os.path.join(base_dir, \"partial\"),\n",
    "        os.path.join(base_dir, \"by_dataset\"),\n",
    "        os.path.join(base_dir, \"by_algorithm\")\n",
    "    ]\n",
    "    for d in dirs: os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    print(f\"Session 2 Runner Started: {session_id}\")\n",
    "    print(\"Generating task list...\")\n",
    "    all_tasks = generate_task_list()\n",
    "\n",
    "    if not all_tasks:\n",
    "        print(\"No tasks configured.\")\n",
    "        return\n",
    "\n",
    "    global_results = []\n",
    "    current_ds_results = []\n",
    "    current_ds_name = None\n",
    "    X, y = None, None\n",
    "\n",
    "    pbar = tqdm(all_tasks, unit=\"exp\")\n",
    "\n",
    "    for i, task in enumerate(pbar):\n",
    "        ds_name = task[\"dataset\"]\n",
    "        desc = f\"{ds_name} | {task.get('algo_name')} | k={task['n_clusters']}\"\n",
    "        pbar.set_description(f\"{desc:<45}\")\n",
    "\n",
    "        if ds_name != current_ds_name:\n",
    "            if current_ds_name and current_ds_results:\n",
    "                save_dataframe(current_ds_results, dirs[2], f\"{current_ds_name}_results.csv\")\n",
    "                current_ds_results = []\n",
    "            try:\n",
    "                X, y, _ = preprocess_single_arff(DATASETS_MAP[ds_name], drop_class=False)\n",
    "                current_ds_name = ds_name\n",
    "            except Exception as e:\n",
    "                pbar.write(f\"Error loading {ds_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        res = {}\n",
    "\n",
    "        try:\n",
    "            if task[\"type\"] == \"kmeans\":\n",
    "                # Prepare arguments dynamically\n",
    "                kwargs = {\n",
    "                    \"n_clusters\": task[\"n_clusters\"],\n",
    "                    \"random_state\": task[\"run_id\"]\n",
    "                }\n",
    "\n",
    "                # Add metric OR kernel depending on what the task specifies\n",
    "                if \"kernel\" in task:\n",
    "                    kwargs[\"kernel\"] = task[\"kernel\"]\n",
    "                elif \"metric\" in task:\n",
    "                    kwargs[\"metric\"] = task[\"metric\"]\n",
    "\n",
    "                # Initialize and run\n",
    "                model = task[\"class\"](**kwargs)\n",
    "                labels = model.fit_predict(X)\n",
    "\n",
    "                res = {\n",
    "                    \"dataset\": ds_name,\n",
    "                    \"algorithm\": task[\"algo_name\"],\n",
    "                    \"n_clusters\": task[\"n_clusters\"],\n",
    "                    \"run_id\": task[\"run_id\"],\n",
    "                    \"inertia\": getattr(model, 'inertia_', 0),\n",
    "                    \"labels\": labels\n",
    "                }\n",
    "                # Log the metric/kernel used\n",
    "                if \"metric\" in task: res[\"metric\"] = task[\"metric\"]\n",
    "                if \"kernel\" in task: res[\"kernel\"] = task[\"kernel\"]\n",
    "\n",
    "            elif task[\"type\"] == \"fuzzy\":\n",
    "                fcm = FuzzyCMeans(\n",
    "                    n_clusters=task[\"n_clusters\"],\n",
    "                    m=task[\"m\"],\n",
    "                    alpha=task[\"alpha\"],\n",
    "                    random_state=task[\"run_id\"]\n",
    "                )\n",
    "                labels = fcm.fit_predict(X)\n",
    "                res = {\n",
    "                    \"dataset\": ds_name,\n",
    "                    \"algorithm\": task[\"algo_name\"],\n",
    "                    \"n_clusters\": task[\"n_clusters\"],\n",
    "                    \"metric\": \"euclidean\",  # FCM is L2 based\n",
    "                    \"param_m\": task[\"m\"],\n",
    "                    \"param_alpha\": task[\"alpha\"],\n",
    "                    \"run_id\": task[\"run_id\"],\n",
    "                    \"labels\": labels\n",
    "                }\n",
    "\n",
    "            res[\"runtime\"] = time.perf_counter() - start_time\n",
    "            if y is not None and \"labels\" in res:\n",
    "                res.update(compute_clustering_metrics(X, y, res[\"labels\"]))\n",
    "                del res[\"labels\"]\n",
    "\n",
    "            global_results.append(res)\n",
    "            current_ds_results.append(res)\n",
    "\n",
    "        except Exception as e:\n",
    "            pbar.write(f\"Task failed: {task} Error: {e}\")\n",
    "\n",
    "        if (i + 1) % PARTIAL_SAVE_INTERVAL == 0:\n",
    "            save_dataframe(global_results, dirs[1], f\"partial_{session_id}.csv\")\n",
    "\n",
    "    if current_ds_results:\n",
    "        save_dataframe(current_ds_results, dirs[2], f\"{current_ds_name}_results.csv\")\n",
    "\n",
    "    if global_results:\n",
    "        df_final = pd.DataFrame(global_results)\n",
    "        df_final.to_csv(os.path.join(base_dir, \"session2_final_results.csv\"), index=False)\n",
    "\n",
    "        for algo in df_final['algorithm'].unique():\n",
    "            safe_name = algo.replace(\" \", \"_\")\n",
    "            save_dataframe(df_final[df_final['algorithm'] == algo], dirs[3], f\"{safe_name}.csv\")\n",
    "\n",
    "        print(f\"\\nSession 2 Complete. Results in {base_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
