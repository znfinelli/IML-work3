{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Utilities\n",
    "from utils.parser import preprocess_single_arff\n",
    "from utils.clustering_metrics import compute_clustering_metrics\n",
    "\n",
    "# Session 3 Algorithms/Reduction\n",
    "from algorithms.pca import PCA\n",
    "from algorithms.kmeansfekm import KMeansFEKM\n",
    "from algorithms.kernel_kmeans import KernelKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ---------------------------------------------------------\n",
    "RUN_CONFIG = {\n",
    "    \"datasets\": {\n",
    "        \"pen-based\": False,\n",
    "        \"adult\": True,\n",
    "        \"mushroom\": False\n",
    "    },\n",
    "    \"algorithms\": {\n",
    "        \"KMeansFEKM\": False,\n",
    "        \"KernelKMeans\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "DATASETS_MAP = {\n",
    "    \"pen-based\": \"datasets/pen-based.arff\",\n",
    "    \"adult\": \"datasets/adult.arff\",\n",
    "    \"mushroom\": \"datasets/mushroom.arff\",\n",
    "}\n",
    "\n",
    "# PCA Configuration\n",
    "# We test specific lower dimensions (2D, 3D for viz, 5D for mild reduction)\n",
    "PCA_COMPONENTS_LIST = [2, 3, 5]\n",
    "\n",
    "# Clustering Configuration\n",
    "N_CLUSTERS_LIST = list(range(2, 11))\n",
    "N_RUNS = 5  # Reduced runs for PCA experiments to save time (Deterministic PCA)\n",
    "PARTIAL_SAVE_INTERVAL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------------------------------------\n",
    "def generate_task_list():\n",
    "    tasks = []\n",
    "    for ds_name, ds_enabled in RUN_CONFIG[\"datasets\"].items():\n",
    "        if not ds_enabled: continue\n",
    "\n",
    "        # Iterate through target dimensions\n",
    "        for n_comp in PCA_COMPONENTS_LIST:\n",
    "\n",
    "            # 1. FEKM Tasks on Reduced Data\n",
    "            if RUN_CONFIG[\"algorithms\"][\"KMeansFEKM\"]:\n",
    "                for k in N_CLUSTERS_LIST:\n",
    "                    # FEKM is deterministic, so 1 run is enough per configuration\n",
    "                    tasks.append({\n",
    "                        \"algorithm\": \"KMeans_FEKM\",\n",
    "                        \"class\": KMeansFEKM,\n",
    "                        \"dataset\": ds_name,\n",
    "                        \"n_components\": n_comp,\n",
    "                        \"n_clusters\": k,\n",
    "                        \"metric\": \"euclidean\",\n",
    "                        \"run_id\": 0\n",
    "                    })\n",
    "\n",
    "            # 2. Kernel K-Means Tasks on Reduced Data\n",
    "            if RUN_CONFIG[\"algorithms\"][\"KernelKMeans\"]:\n",
    "                for k in N_CLUSTERS_LIST:\n",
    "                    # Intelligent Kernel KMeans is also deterministic\n",
    "                    tasks.append({\n",
    "                        \"algorithm\": \"Kernel_KMeans\",\n",
    "                        \"class\": KernelKMeans,\n",
    "                        \"dataset\": ds_name,\n",
    "                        \"n_components\": n_comp,\n",
    "                        \"n_clusters\": k,\n",
    "                        \"kernel\": \"rbf\",  # Use RBF as standard for Kernel KM\n",
    "                        \"run_id\": 0\n",
    "                    })\n",
    "    return tasks\n",
    "\n",
    "\n",
    "def save_dataframe(data, folder, filename):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        if data.empty: return\n",
    "        df_to_save = data\n",
    "    elif not data:\n",
    "        return\n",
    "    else:\n",
    "        df_to_save = pd.DataFrame(data)\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    df_to_save.to_csv(os.path.join(folder, filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Main Execution Loop\n",
    "# ---------------------------------------------------------\n",
    "def main():\n",
    "    session_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_dir = f\"results_session3/run_{session_id}\"\n",
    "    dirs = [\n",
    "        base_dir,\n",
    "        os.path.join(base_dir, \"partial\"),\n",
    "        os.path.join(base_dir, \"by_dataset\"),\n",
    "        os.path.join(base_dir, \"by_algorithm\")\n",
    "    ]\n",
    "    for d in dirs: os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    print(f\"Session 3 Runner (PCA + Clustering) Started: {session_id}\")\n",
    "    print(\"Generating task list...\")\n",
    "    all_tasks = generate_task_list()\n",
    "\n",
    "    if not all_tasks:\n",
    "        print(\"No tasks configured.\")\n",
    "        return\n",
    "\n",
    "    global_results = []\n",
    "    current_ds_results = []\n",
    "    current_ds_name = None\n",
    "\n",
    "    # Cache for PCA-transformed data to avoid re-computing PCA for every K\n",
    "    # Format: { (dataset_name, n_components): X_reduced }\n",
    "    pca_cache = {}\n",
    "\n",
    "    # Raw data cache\n",
    "    X_orig, y_orig = None, None\n",
    "\n",
    "    pbar = tqdm(all_tasks, unit=\"exp\")\n",
    "\n",
    "    for i, task in enumerate(pbar):\n",
    "        ds_name = task[\"dataset\"]\n",
    "        n_comp = task[\"n_components\"]\n",
    "        algo_name = task[\"algorithm\"]\n",
    "\n",
    "        desc = f\"{ds_name} | PCA({n_comp}D) -> {algo_name} | k={task['n_clusters']}\"\n",
    "        pbar.set_description(f\"{desc:<60}\")\n",
    "\n",
    "        # 1. Load Original Data if needed\n",
    "        if ds_name != current_ds_name:\n",
    "            # Save previous dataset results\n",
    "            if current_ds_name and current_ds_results:\n",
    "                save_dataframe(current_ds_results, dirs[2], f\"{current_ds_name}_results.csv\")\n",
    "                current_ds_results = []\n",
    "                pca_cache = {}  # Clear PCA cache for new dataset\n",
    "\n",
    "            try:\n",
    "                # Load fresh data\n",
    "                X_orig, y_orig, _ = preprocess_single_arff(DATASETS_MAP[ds_name], drop_class=False)\n",
    "                current_ds_name = ds_name\n",
    "            except Exception as e:\n",
    "                pbar.write(f\"Error loading {ds_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # 2. Perform or Retrieve PCA Reduction\n",
    "        # We check if we can actually reduce to n_components (e.g. can't reduce 4 dims to 5)\n",
    "        if n_comp >= X_orig.shape[1]:\n",
    "            # Skip invalid reduction requests\n",
    "            continue\n",
    "\n",
    "        if n_comp not in pca_cache:\n",
    "            # Run Custom PCA\n",
    "            try:\n",
    "                pca = PCA(n_components=n_comp)\n",
    "                X_reduced = pca.fit_transform(X_orig)\n",
    "                pca_cache[n_comp] = X_reduced\n",
    "            except Exception as e:\n",
    "                pbar.write(f\"PCA Failed for {ds_name} ({n_comp}D): {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            X_reduced = pca_cache[n_comp]\n",
    "\n",
    "        # 3. Run Clustering on Reduced Data\n",
    "        start_time = time.perf_counter()\n",
    "        res = {}\n",
    "\n",
    "        try:\n",
    "            # Prepare arguments\n",
    "            kwargs = {\n",
    "                \"n_clusters\": task[\"n_clusters\"],\n",
    "                \"random_state\": task[\"run_id\"]\n",
    "            }\n",
    "\n",
    "            # Add algo-specific params\n",
    "            if \"kernel\" in task:\n",
    "                kwargs[\"kernel\"] = task[\"kernel\"]\n",
    "            if \"metric\" in task:\n",
    "                kwargs[\"metric\"] = task[\"metric\"]\n",
    "\n",
    "            # Initialize and Fit\n",
    "            model = task[\"class\"](**kwargs)\n",
    "\n",
    "            # fit_predict on X_reduced\n",
    "            labels = model.fit_predict(X_reduced)\n",
    "\n",
    "            # Record Results\n",
    "            res = {\n",
    "                \"dataset\": ds_name,\n",
    "                \"preprocessing\": f\"PCA_{n_comp}D\",\n",
    "                \"n_features_orig\": X_orig.shape[1],\n",
    "                \"n_features_reduced\": n_comp,\n",
    "                \"algorithm\": algo_name,\n",
    "                \"n_clusters\": task[\"n_clusters\"],\n",
    "                \"run_id\": task[\"run_id\"],\n",
    "                \"inertia\": getattr(model, 'inertia_', 0),\n",
    "                \"runtime\": time.perf_counter() - start_time,\n",
    "                \"labels\": labels\n",
    "            }\n",
    "\n",
    "            # Compute Metrics using ORIGINAL Ground Truth (y_orig)\n",
    "            # Note: DBI is calculated on X_reduced to evaluate cluster compactness in the new space\n",
    "            if y_orig is not None:\n",
    "                metrics = compute_clustering_metrics(X_reduced, y_orig, labels)\n",
    "                res.update(metrics)\n",
    "                del res[\"labels\"]  # Remove labels to save space\n",
    "\n",
    "            global_results.append(res)\n",
    "            current_ds_results.append(res)\n",
    "\n",
    "        except Exception as e:\n",
    "            pbar.write(f\"Clustering Failed: {task} Error: {e}\")\n",
    "\n",
    "        # Partial Save\n",
    "        if (i + 1) % PARTIAL_SAVE_INTERVAL == 0:\n",
    "            save_dataframe(global_results, dirs[1], f\"partial_{session_id}.csv\")\n",
    "\n",
    "    # Final Saves\n",
    "    if current_ds_results:\n",
    "        save_dataframe(current_ds_results, dirs[2], f\"{current_ds_name}_results.csv\")\n",
    "\n",
    "    if global_results:\n",
    "        df_final = pd.DataFrame(global_results)\n",
    "        df_final.to_csv(os.path.join(base_dir, \"session3_final_results.csv\"), index=False)\n",
    "\n",
    "        for algo in df_final['algorithm'].unique():\n",
    "            safe_name = algo.replace(\" \", \"_\")\n",
    "            save_dataframe(df_final[df_final['algorithm'] == algo], dirs[3], f\"{safe_name}.csv\")\n",
    "\n",
    "        print(f\"\\nSession 3 Complete. Results in {base_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
